{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D1 - Gather as much medical text as possible and learn embeddings from those texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get/Scrape the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of ways of scrapping a web to recollect *.pdf files, one which most of webs can't block is a Web Browser Automation like Selenium, you can create a script to automate some tasks, but since Pubmed have an API is easier using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scrape and recolect documents we use Entrez, which helps us to get the IDs from our search, which will be free papers in english, and when we have the PMIDs, we will use a library to automatize the search and download of that papers.\n",
    "If we don't want to use this library, we need to automatize the download of the pdf files for the different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    Entrez.email = 'set.tobur@gmail.com' #Entrez needs an email for avoid bots\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax='20000',\n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'set.tobur@gmail.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Update: Plant Cortical Microtubule Arrays.\n",
      "2) Arabidopsis VPS38 is required for vacuolar trafficking but dispensable for autophagy.\n",
      "3) Precision genome editing using synthesis-dependent repair of Cas9-induced DNA breaks.\n",
      "4) Robust zero resistance in a superconducting high-entropy alloy at pressures up to 190 GPa.\n",
      "5) Vasopressin excites interneurons to suppress hippocampal network activity across a broad span of brain maturity at birth.\n",
      "6) Osmosensing by the bacterial PhoQ/PhoP two-component system.\n",
      "7) On the role of the corpus callosum in interhemispheric functional connectivity in humans.\n",
      "8) Inequality in nature and society.\n",
      "9) Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States.\n",
      "10) Oral anticoagulants for prevention of stroke in atrial fibrillation: systematic review, network meta-analysis, and cost effectiveness analysis.\n",
      "11) Marriage and risk of dementia: systematic review and meta-analysis of observational studies.\n",
      "12) Total workload and recovery in relation to worktime reduction: a randomised controlled intervention study with time-use data.\n",
      "13) Anisotropic shear stress patterns predict the orientation of convergent tissue movements in the embryonic heart.\n",
      "14) Managing alcohol-related attendances in emergency care: can diversion to bespoke services lessen the burden?\n",
      "15) Implementation science at the crossroads.\n",
      "16) MiR-146a Aggravates LPS-Induced Inflammatory Injury by Targeting CXCR4 in the Articular Chondrocytes.\n",
      "17) Efficacy of a New Post-Mouthwash Intervention (Wiping Plus Oral Nutritional Supplements) for Preventing Aspiration Pneumonia in Elderly People: A Multicenter, Randomized, Comparative Trial.\n",
      "18) Identification of Subpathway Signatures For Ovarian Cancer Prognosis by Integrated Analyses of High-Throughput miRNA and mRNA Expression.\n",
      "19) Uremic Pruritus: An Itch with Ominous Consequences.\n",
      "20) Transoral Robotic Surgery with Sialendoscopy for a Plunging Ranula.\n",
      "21) Hypoxia-Induced Mesenchymal Stromal Cells Exhibit an Enhanced Therapeutic Effect on Radiation-Induced Lung Injury in Mice due to an Increased Proliferation Potential and Enhanced Antioxidant Ability.\n",
      "22) MiR-193a-3p is an Important Tumour Suppressor in Lung Cancer and Directly Targets KRAS.\n",
      "23) Microarray Expression Profile of Circular RNAs in Plasma from Primary Biliary Cholangitis Patients.\n",
      "24) Effect of polymyxin B-containing regimens on renal function for the treatment of carbapenem-resistant Enterobacteriacea mediastinitis.\n",
      "25) Epidermolysis Bullosa Acquisita in an Adult Patient with Previously Unrecognized Mild Dystrophic EB and Biallelic COL7A1 Mutations.\n",
      "26) Atypical Hand, Foot, and Mouth Disease Caused by Coxsackievirus A6 in Denmark: A Diagnostic Mimicker.\n",
      "27) Substance P Antagonist Aprepitant Shows no Additive Effect Compared with Standardized Topical Treatment Alone in Patients with Atopic Dermatitis.\n",
      "28) Incidence of Actinic Keratosis and Risk of Skin Cancer in Subjects with Actinic Keratosis: A Population-based Cohort Study.\n",
      "29) Semi-automatic tracking, smoothing and segmentation of hyoid bone motion from videofluoroscopic swallowing study.\n",
      "30) miR205 inhibits stem cell renewal in SUM159PT breast cancer cells.\n",
      "31) Principal component and discriminant analyses as powerful tools to support taxonomic identification and their use for functional and phylogenetic signal detection of isolated fossil shark teeth.\n",
      "32) Using 13C in cattle hair to trace back the maize level in the feeding regime-A field test.\n",
      "33) The extracellular matrix protein Edil3 stimulates osteoblast differentiation through the integrin α5β1/ERK/Runx2 pathway.\n",
      "34) Arabidopsis ubiquitin ligase PUB12 interacts with and negatively regulates Chitin Elicitor Receptor Kinase 1 (CERK1).\n",
      "35) An analytical approach to sparse telemetry data.\n",
      "36) Ensemble machine learning and forecasting can achieve 99% uptime for rural handpumps.\n",
      "37) Clustering of samples and variables with mixed-type data.\n",
      "38) NKT cells contribute to basal IL-4 production but are not required to induce experimental asthma.\n",
      "39) Heat shock proteins HSPB8 and DNAJC5B have HCV antiviral activity.\n",
      "40) The role of protease inhibitors on the remineralization of demineralized dentin using the PILP method.\n",
      "41) Inattention in primary school is not good for your future school achievement-A pattern classification study.\n",
      "42) Classical and next generation sequencing approaches unravel Bymovirus diversity in barley crops in France.\n",
      "43) Fractal analyses reveal independent complexity and predictability of gait.\n",
      "44) Accuracy and reproducibility of CT right-to-left ventricular diameter measurement in patients with acute pulmonary embolism.\n",
      "45) A mechanistic model for spread of livestock-associated methicillin-resistant Staphylococcus aureus (LA-MRSA) within a pig herd.\n",
      "46) Recovery from trauma induced amnesia correlates with normalization of thrombin activity in the mouse hippocampus.\n",
      "47) Evidence of increased toxic Alexandrium tamarense dinoflagellate blooms in the eastern Bering Sea in the summers of 2004 and 2005.\n",
      "48) Men's perspectives of prostate cancer screening: A systematic review of qualitative studies.\n",
      "49) Influence of different feeding regimes on the survival, growth, and biochemical composition of Acropora coral recruits.\n",
      "50) HLA and non-HLA genes and familial predisposition to autoimmune diseases in families with a child affected by type 1 diabetes.\n",
      "51) How do adults and teens with self-declared Autism Spectrum Disorder experience eye contact? A qualitative analysis of first-hand accounts.\n",
      "52) Treatment eligibility and retention in clinical HIV care: A regression discontinuity study in South Africa.\n",
      "53) Mycophenolate mofetil prevents the delayed T cell response after pilocarpine-induced status epilepticus in mice.\n",
      "54) Local ecological knowledge and its relationship with biodiversity conservation among two Quilombola groups living in the Atlantic Rainforest, Brazil.\n",
      "55) The evolution and adaptation of A-to-I RNA editing.\n",
      "56) Extensive virologic and immunologic characterization in an HIV-infected individual following allogeneic stem cell transplant and analytic cessation of antiretroviral therapy: A case study.\n",
      "57) The expanding epidemic of HIV-1 in the Russian Federation.\n",
      "58) Effects of peroxisome proliferator activated receptors (PPAR)-γ and -α agonists on cochlear protection from oxidative stress.\n",
      "59) Human papillomavirus DNA detection in plasma and cervical samples of women with a recent history of low grade or precancerous cervical dysplasia.\n",
      "60) Regional brain morphometry in patients with traumatic brain injury based on acute- and chronic-phase magnetic resonance imaging.\n",
      "61) Immunotherapeutic efficacy of liposome-encapsulated refined allergen vaccines against Dermatophagoides pteronyssinus allergy.\n",
      "62) Exploring the reproducibility of functional connectivity alterations in Parkinson's disease.\n",
      "63) Response of imported malaria patients to antimalarial medicines in Sri Lanka following malaria elimination.\n",
      "64) Analytical Methods of Phytochemicals from the Genus Gentiana.\n",
      "65) ECG Signal De-noising and Baseline Wander Correction Based on CEEMDAN and Wavelet Threshold.\n",
      "66) Radio-Frequency-Controlled Urea Dosing for NH₃-SCR Catalysts: NH₃ Storage Influence to Catalyst Performance under Transient Conditions.\n",
      "67) Recent Advances in Momordica charantia: Functional Components and Biological Activities.\n",
      "68) Optimization and Characterization of Paper-Made Surface Enhanced Raman Scattering (SERS) Substrates with Au and Ag NPs for Quantitative Analysis.\n",
      "69) trans-Double Bond-Containing Liposomes as Potential Carriers for Drug Delivery.\n",
      "70) Harvesting a 3D N-Doped Carbon Network from Waste Bean Dregs by Ionothermal Carbonization as an Electrocatalyst for an Oxygen Reduction Reaction.\n",
      "71) Antarctic Krill Oil Diet Protects against Lipopolysaccharide-Induced Oxidative Stress, Neuroinflammation and Cognitive Impairment.\n",
      "72) Estrogen Metabolism-Associated CYP2D6 and IL6-174G/C Polymorphisms in Schistosoma haematobium Infection.\n",
      "73) Modification of Bacterial Cellulose Biofilms with Xylan Polyelectrolytes.\n",
      "74) Room-Temperature H₂ Gas Sensing Characterization of Graphene-Doped Porous Silicon via a Facile Solution Dropping Method.\n",
      "75) Graphene Oxide-Silver Nanoparticles Nanocomposite Stimulates Differentiation in Human Neuroblastoma Cancer Cells (SH-SY5Y).\n",
      "76) Knee Impedance Modulation to Control an Active Orthosis Using Insole Sensors.\n",
      "77) Mathematical Model for Localised and Surface Heat Flux of the Human Body Obtained from Measurements Performed with a Calorimetry Minisensor.\n",
      "78) PPARγ Modulates Long Chain Fatty Acid Processing in the Intestinal Epithelium.\n",
      "79) Development of a High Precision Displacement Measurement System by Fusing a Low Cost RTK-GPS Sensor and a Force Feedback Accelerometer for Infrastructure Monitoring.\n",
      "80) Analysis of MicroRNA Expression in Newborns with Differential Birth Weight Using Newborn Screening Cards.\n",
      "81) Study of Perfluorophosphonic Acid Surface Modifications on Zinc Oxide Nanoparticles.\n",
      "82) Effect of Dietary Acidolysis-Oxidized Konjac Glucomannan Supplementation on Serum Immune Parameters and Intestinal Immune-Related Gene Expression of Schizothorax prenanti.\n",
      "83) Characterization and Computation of Yb/TiO₂ and Its Photocatalytic Degradation with Benzohydroxamic Acid.\n",
      "84) Coumarin Derivatives Solvent-Free Synthesis under Microwave Irradiation over Heterogeneous Solid Catalysts.\n",
      "85) Transparent and Flexible Capacitors with an Ultrathin Structure by Using Graphene as Bottom Electrodes.\n",
      "86) Effects of Group Counseling Programs, Cognitive Behavioral Therapy, and Sports Intervention on Internet Addiction in East Asia: A Systematic Review and Meta-Analysis.\n",
      "87) Gene Silencing of Argonaute5 Negatively Affects the Establishment of the Legume-Rhizobia Symbiosis.\n",
      "88) Detection of Abrin Holotoxin Using Novel Monoclonal Antibodies.\n",
      "89) Home Hemodialysis (HHD) Treatment as Effective yet Underutilized Treatment Modality in the United States.\n",
      "90) Ceramic-Based 4D Components: Additive Manufacturing (AM) of Ceramic-Based Functionally Graded Materials (FGM) by Thermoplastic 3D Printing (T3DP).\n",
      "91) Effect of Composition and Thickness on the Perpendicular Magnetic Anisotropy of (Co/Pd) Multilayers.\n",
      "92) Dynamics of Pathological and Virological Findings During Experimental Calpox Virus Infection of Common Marmosets (Callithrix jacchus).\n",
      "93) Contextual Exploration of a New Family Caregiver Support Concept for Geriatric Settings Using a Participatory Health Research Strategy.\n",
      "94) Early Gabapentin Treatment during the Latency Period Increases Convulsive Threshold, Reduces Microglial Activation and Macrophage Infiltration in the Lithium-Pilocarpine Model of Epilepsy.\n",
      "95) Different Facets of Body Image Disturbance in Binge Eating Disorder: A Review.\n",
      "96) Placing Salt/Soy Sauce at Dining Tables and Out-Of-Home Behavior Are Related to Urinary Sodium Excretion in Japanese Secondary School Students.\n",
      "97) All-Fiber Laser Curvature Sensor Using an In-Fiber Modal Interferometer Based on a Double Clad Fiber and a Multimode Fiber Structure.\n",
      "98) Climatic Factors and Influenza Transmission, Spain, 2010-2015.\n",
      "99) Validation of a claims-based algorithm to characterize episodes of care.\n",
      "100) Changes in cardiovascular care provision after the Affordable Care Act.\n"
     ]
    }
   ],
   "source": [
    "results = search('\"english\"[Language] AND free full text[sb]')\n",
    "id_list = results['IdList']\n",
    "papers = fetch_details(id_list)\n",
    "for i, paper in enumerate(papers['PubmedArticle']):\n",
    "    print(\"%d) %s\" % (i+1, paper['MedlineCitation']['Article']['ArticleTitle']))\n",
    "# Pretty print the first paper in full to observe its structure\n",
    "#import json\n",
    "#print(json.dumps(papers[0], indent=2, separators=(',', ':')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can use the PMID to use some library and downdload papers\n",
    "lista_PMIDS = \"\"\n",
    "for i, paper in enumerate(papers['PubmedArticle']):\n",
    "    lista_PMIDS += str(paper['MedlineCitation']['PMID']+',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_PMIDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we get all the PMIDS we can call the script and try to get all the papers\n",
    "\n",
    "You can get the install instructions from https://github.com/billgreenwald/Pubmed-Batch-Download, but for me worked better a manual installation.\n",
    "\n",
    " Install ruby & ruby-dev from apt-get or pacman and then write in a terminal:\n",
    " - gem install mechanize camping socksify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "cmd = 'ruby Pubmed-Batch-Download/pubmedid2pdf.rb %s' % (lista_PMIDS)\n",
    "run = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out, err = run.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the documents, we need to parse all the pdf files to text file for training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdfparser.poppler as pdf\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def remove_accents(text, method='unicode'):\n",
    "    \"\"\"\n",
    "    http://textacy.readthedocs.io/en/latest/_modules/textacy/preprocess.html\n",
    "        if 'unicode', remove accented char for any unicode symbol with a \n",
    "        direct ASCII equivalent; \n",
    "        if 'ascii', remove accented char for any unicode symbol\n",
    "\n",
    "        NB: the 'ascii' method is notably faster than 'unicode', but less good\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'unicode':\n",
    "        return ''.join(c for c in unicodedata.normalize('NFKD', text)\n",
    "                       if not unicodedata.combining(c))\n",
    "    elif method == 'ascii':\n",
    "        return unicodedata.normalize('NFKD', text).encode('ascii', errors='ignore').decode('ascii')\n",
    "    else:\n",
    "        msg = '`method` must be either \"unicode\" and \"ascii\", not {}'.format(method)\n",
    "        raise ValueError(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_superscript(text):\n",
    "    m = re.search('[a-z][a-z][0-9] ', text)\n",
    "    if m:        \n",
    "        text = text[:m.start()+2] + text[m.start() + 3:]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    doc = pdf.Document(str.encode(file))\n",
    "    \n",
    "    phrases = []\n",
    "    #print('No of pages', doc.no_of_pages)\n",
    "    for page in doc:\n",
    "        for flow in page:\n",
    "            for box in flow:\n",
    "                box.bbox.as_tuple()                \n",
    "                for line in box:\n",
    "                    text_cleaned = remove_accents(line.text)\n",
    "                    text_cleaned = remove_superscript(text_cleaned)\n",
    "                    phrases.append(text_cleaned)\n",
    "                    #phrases.append(line.text.encode('UTF-8'))\n",
    "    return phrases\n",
    "#load_file('../data/external/medicalPapersBatch1_011017/es_1678-4464-csp-33-s3-e00104917.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import queue\n",
    "#save pdf files\n",
    "def save_documents(pathInput, pathOutput='', where=0):\n",
    "    if where == 0: # save to a data structure\n",
    "        all_docs = []\n",
    "        for file in os.listdir(pathInput):\n",
    "            all_docs.append(load_file(os.path.join(pathInput, file)))\n",
    "        return all_docs\n",
    "    else: #save to file in disk\n",
    "        q = queue.Queue()\n",
    "        s = \"\"\n",
    "        for file in tqdm(os.listdir(pathInput)):\n",
    "            fileToSave = open(os.path.join(pathOutput, file), \"w\")\n",
    "            values = load_file(os.path.join(pathInput, file))\n",
    "            with fileToSave as myfile:\n",
    "                for line in values:\n",
    "                    if re.match('(.)*-$',line):                        \n",
    "                        #we create a FIFO queue for join lines with dashes\n",
    "                        q.put(line[:-1])\n",
    "                    else:\n",
    "                        #while there are not dashes we join the lines\n",
    "                        while not q.empty():\n",
    "                            s += q.get()                                                        \n",
    "                        #print('{}{}\\n'.format(s,line))\n",
    "                        myfile.write('{}{}\\n'.format(s,line))\n",
    "                        s = \"\" # If script gets here we empty the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:19<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "all_docs = save_documents('data/pdf/','data/processed/', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, all of this is only a Proof of Concept, because in real life you will need to recollect a lot more of documents and parse much better, to create a clean corpus, because for example right now it has a lot of numbers combined with letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        punctuations = list(string.punctuation)\n",
    "        stop = stopwords.words('spanish') + stopwords.words('english') + punctuations\n",
    "        stop += ['et', '1', '2', 'j', '3']        \n",
    "        for fname in tqdm(os.listdir(self.dirname)):\n",
    "            for line in open(os.path.join(self.dirname, fname), \"rb\"):\n",
    "                sentence = line.decode('utf-8').lower()\n",
    "                parts = nltk.word_tokenize(sentence)\n",
    "                important_words = [item for item in parts if item not in stop]\n",
    "                yield important_words            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = MySentences('data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:21<00:00,  2.46it/s]\n",
      "100%|██████████| 53/53 [00:22<00:00,  2.38it/s]\n",
      "100%|██████████| 53/53 [00:22<00:00,  2.39it/s]\n",
      "100%|██████████| 53/53 [00:23<00:00,  2.24it/s]\n",
      "100%|██████████| 53/53 [00:26<00:00,  2.03it/s]\n",
      "100%|██████████| 53/53 [00:23<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, sg = 1, min_count=4, size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model.save('models/w2v_v2.bin') we can save the model for retraining or use it in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since is very hard to get a clean corpus and train models, I decide to use a pretrained model, trained from PubMed and PMC from http://bio.nlplab.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When we use the method load_word2vec_format() we can't retrain the model\n",
    "model = KeyedVectors.load_word2vec_format('models/PubMed-and-PMC-w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this we will use a paper called \"How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks\" and their code, which can find in https://github.com/kudkudak/word-embeddings-benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMNSRS\n",
    ":  The  University  of  Minnesota  Semantic  Relatedness  Set  (UMNSRS)  was  developed by Pakhomov et al. (2010), and consists of 725 clinical term pairs whose semantic similarity and relatedness was determined independently by four medical residents from the University of Minnesota Medical School.  The similarity and relatedness of each term pair was annotated based on a continuous scale by having the resident touch a baron a touch sensitive computer screen to indicate the degree of similarity or relatedness.  The Intraclass Correlation Coefficient (ICC) for the reference standard tagged for similarity was 0.47, and 0.50 for relatedness.   Therefore,  as suggested by Pakhomov and colleagues,we use a subset of the ratings  consisting  of  401  pairs  for  the  similarity set and 430 pairs for the relatedness set which each have an ICC of 0.73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "cmd = 'scripts/word-embeddings-benchmarks/scripts/evaluate_on_all.py --file ~/jupyterNotebooks/mlnn2017/P3\\ -\\ Exploring\\ specific\\ domains\\ with\\ embeddings/models/PubMed-and-PMC-w2v.bin --format word2vec_bin'\n",
    "run = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out, err = run.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://kokes.github.io/nbviewer.js/viewer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|                                | AP    | Battig | ESSLI_2b | ESSLI_2c | MEN   | MTurk | SimLex999 | WS353 | WS353S | UMNSRS_similarity | UMNSRS_relatedness |\n",
    "|--------------------------------|-------|--------|----------|----------|-------|-------|-----------|-------|--------|-------------------|--------------------|\n",
    "| PubMed-and-PMC-w2v             | 0.554 | 0.308  | 0.725    | 0.488    | 0.545 | 0.446 | 0.261     | 0.416 | 0.475  | 0.541             | 0.4864             |\n",
    "| SBW-vector-300-min5+medics     | 0.281 | 0.246  | 0.55     | 0.4      | 0.314 | 0.232 | 0.145     | 0.232 | 0.303  | 0.059             | 0.033              |\n",
    "| SBW-vectors-300-min5           | 0.325 | 0.239  | 0.425    | 0.377    | 0.32  | 0.262 | 0.128     | 0.280 | 0.332  | 0.053             | 0.027              |\n",
    "| LexVec which=\"commoncrawl-W+C\" | 0.639 | 0.431  | 0.725    | 0.644    | 0.809 | 0.712 | 0.419     | 0.647 | 0.756  |                   |                    |\n",
    "| GloVe dim=300 corpus=wiki-6B   | 0.622 | 0.451  | 0.750    | 0.578    | 0.737 | 0.633 | 0.371     | 0.522 | 0.653  | 0.261             | 0.243              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2 - Explore whether a combination with general domain corpora improves the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we could scrape wikipedia with BeautifulSoup4 or Selenium. This will be easier than with the pdf files, since HTML is a text format, and it gives us all the information we need about the text, but since the BioNLP group had already the previous model with a dump of the Wikipedia, we are going to use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a model, and we wanted to retrain it with a bigger corpus, or like in this case in other domain, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We load the simple model\n",
    "simple_model = KeyedVectors.load_word2vec_format('models/PubMed-and-PMC-w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We set the directory of the new files for the retraining\n",
    "dirpath='data/processed/'\n",
    "corpusReader = nltk.corpus.PlaintextCorpusReader(dirpath, '.*\\.txt')\n",
    "frases = len(corpusReader.sents())\n",
    "print(\"The number of sentences =\", frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = MySentences(dirpath)\n",
    "model.train(sentences, total_examples=frases, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want we can save our new model retrained with the new corpora\n",
    "#model.wv.save_word2vec_format('../models/model_w2v_sbw_withMedical.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#And now we can load the combined model and checks his accuracy\n",
    "combined_model = KeyedVectors.load_word2vec_format('models/wikipedia-pubmed-and-PMC-w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|                                | AP    | Battig | ESSLI_2b | ESSLI_2c | MEN   | MTurk | SimLex999 | WS353 | WS353S | UMNSRS_similarity | UMNSRS_relatedness |\n",
    "|--------------------------------|-------|--------|----------|----------|-------|-------|-----------|-------|--------|-------------------|--------------------|\n",
    "| PubMed-and-PMC-w2v             | 0.554 | 0.308  | 0.725    | 0.488    | 0.545 | 0.446 | 0.261     | 0.416 | 0.475  | 0.541             | 0.4864             |\n",
    "| Wikipedia+PubMed-and-PMC-w2v   | 0.567 | 0.396  | 0.725    | 0.511    | 0.517 | 0.572 | 0.278     | 0.484 | 0.526  | 0.543             | 0.4860             |\n",
    "| SBW-vector-300-min5+medics     | 0.281 | 0.246  | 0.55     | 0.4      | 0.314 | 0.232 | 0.145     | 0.232 | 0.303  | 0.059             | 0.033              |\n",
    "| SBW-vectors-300-min5           | 0.325 | 0.239  | 0.425    | 0.377    | 0.32  | 0.262 | 0.128     | 0.280 | 0.332  | 0.053             | 0.027              |\n",
    "| LexVec which=\"commoncrawl-W+C\" | 0.639 | 0.431  | 0.725    | 0.644    | 0.809 | 0.712 | 0.419     | 0.647 | 0.756  |                   |                    |\n",
    "| GloVe dim=300 corpus=wiki-6B   | 0.622 | 0.451  | 0.750    | 0.578    | 0.737 | 0.633 | 0.371     | 0.522 | 0.653  | 0.261             | 0.243              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see the result in all tests, medical and non-medical are better than with the previous model, so is safe to say is better to train models with a mix of the corpora we are going to have in our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D3: Explore whether the combination with random-walks over knowledge-bases improves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra - Visualization of Embeddings using a dimensionality reduction algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_model = KeyedVectors.load_word2vec_format('models/wikipedia-pubmed-and-PMC-w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"90e93cb1-519e-42f6-8636-f348ef77effa\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '90e93cb1-519e-42f6-8636-f348ef77effa' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '90e93cb1-519e-42f6-8636-f348ef77effa' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"90e93cb1-519e-42f6-8636-f348ef77effa\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.plotting as bp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6d6acc95911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcombined_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtsne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtsne_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \"\"\"\n\u001b[0;32m--> 884\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 707\u001b[0;31m                             dtype=np.float64)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "word_vectors = [combined_model[w] for w in combined_model.wv.vocab.keys()]\n",
    "tsne_model = TSNE(n_components=2, init='pca', verbose=1, random_state=0)\n",
    "tsne_w2v = tsne_model.fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "tsne_df['words'] = combined_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
