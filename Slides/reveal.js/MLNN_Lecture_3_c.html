<!doctype html>
<html lang="en">


   
    
	<head>
		<meta charset="utf-8">

		<title>Machine Learning and Neural Networks</title>
                <meta name="author" content="Roberto Santana">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<!-- <link rel="stylesheet" href="css/reveal.css">  -->
                <link rel="stylesheet" href="css/fullscreen-img.css">
                <link rel="stylesheet" href="css/added_css/notebook.css">
   	        <link rel="stylesheet" href="css/reveal.css">
                <link rel="stylesheet" href="css/theme/nncourse.css" id="theme">
                                

	</head>

	<body>


		<div class="reveal">
			<div class="slides">

				<section>
                                          <div class="my_container">
                                        <h2>Machine Learning and Neural Networks</h2>
					<p>Roberto Santana<p>
					<p>Department of Computer Science and Artificial Intelligence</p>
                                        <p>University of the Basque Country</p>
                          		 </div>   
				</section>

                                 <section>                             	      
                                   <section id="sec:MATH_BG">
                                        <div class="my_container">
                                        <h3>Mathematical background for deep learning: Table of Contents </h3>
                                        
                                         <table style="width:100%"; border=solid>
                                                  <tr>


                                                      <td> <p class="paragraph2"> <a href="#/sec:ML_Models"> ML Models </a></p></td> 
                                                      <td> <p class="paragraph2"> <a href="#/sec:Estimators"> Estimators </a></p></td> 
                                                      <td> <p class="paragraph2"> <a href="#/sec:MLE"> MLE </a></p></td>  
                                                      <td> <p class="paragraph2"> <a href="#/sec:Bayesian_Approach"> Bayesian approach. </a></p></td>
                                                   

                                                   
                                                  </tr>
                                                  <tr>
                                                      <td> <p class="paragraph2"> <a href="#/sec:Regularization"> Regularization </a></p></td>
                                                      <td> <p class="paragraph2"> <a href="#/sec:LossFunctions"> Loss functions </a></p></td>
                                                      <td> <p class="paragraph2"> <a href="#/sec:Gradient"> Gradient </a></p></td>
                                                      <td> <p class="paragraph2"> <a href="#/sec:GradientDescent"> Gradient descent </a></p></td>                                     </tr>             
                                                      
                                    
				         </table>	  
                          	     </div>   


				   </section>

                                   <section id="sec:ML_Models">
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Data generating process and model families </h3>                                                                                         <ul>
  
						          <li class="paragraph2"><mark class="red"> Data generating process</mark>: We assume this is the (true) process from which training and test datasets are generated.</li>
						          <li class="paragraph2"><mark class="red">Data generating distribution</mark>: This is the probability distribution of the data determined by generating process: \(p_{data}\).</li>
                                                  <span class="fragment">
						          <li class="paragraph2"> <mark class="red">Model</mark>: Specifies which functions (<mark class="red"> model family</mark>) the learning algorithm can choose from when varying the parameters in order to approximate the data generating process. </li>

						          <li class="paragraph2"><mark class="red">Model capacity</mark>:  Its capacity to fit a wide variety of functions. </li>
                                                  </span>
                                                  <span class="fragment">
                                                          <li class="paragraph2"><mark class="red">Hypothesis space</mark>:  The set of functions that the learning algorithm is allowed to select as being the solution. </li>
                                                  </span>
                                                   
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                  <section>
                                           <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Data generating process and model families </h3>  
                                                   <h4>Example: Linear models of different capacity  </h4>        

                                                   <ul>
						        <li class="paragraph2"> <mark class="red">Linear regression model</mark>:
                                                          \[
                                                             \hat{y} = b +  w {\bf{x}}
							  \]  
                                                          </li>
						        <li class="paragraph2"> <mark class="red">Linear regression model quadratic</mark> as a function of \( {\bf{x}} \):
                                                          \[
                                                             \hat{y} = b + w_1 {\bf{x}} + w_2 {\bf{x}}^2
							  \]  
                                                          </li>
						        <li class="paragraph2"> <mark class="red">Linear regression model, polynomial</mark> of degree \(9\):
                                                          \[
                                                             \hat{y} = b + \sum_{i=1}^{9}  w_i {\bf{x}}^i
							  \]  
                                                          </li>


                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Data generating process and model families </h3>                                                                                          <ul>

						          <li class="paragraph2">Situations for a <mark class="red">model family</mark> being trained: </li>
                                                              <ol>
						                  <li class="paragraph2"> Excluded the true data generating process.</li>
						                  <li class="paragraph2"> Matched the true data generating process. </li>
						                  <li class="paragraph2"> Included the generating generating process but also many other possible generating processes. </li>
                                                              </ol>
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
 
                                   <section  id="sec:Estimators">
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Estimators</h4>    
                                                   <ul>

						          <li class="paragraph2"> <mark class="red">Point estimation</mark> is the attempt to provide  "the best" prediction for a quantity of interest, e.g., the parameters of a ML model. </li>
					                  <li class="paragraph2"> A  <mark class="red">point estimator</mark> is a any function of the data  \( \hat{\theta}_m = g({\bf{x}}^1, \dots,{\bf{x}}^m) \).</li>
					                  <li class="paragraph2"> A <mark class="red">good estimator</mark> is a function whose output is close to the true underlying \(\theta\) that generated the training data.</li>
					                  <li class="paragraph2">We assume that the true parameter of \(\theta\) is <mark class="red">true but unknown</mark>. Therefore, \( \hat{\theta} \) is a <mark class="red"> random variable</mark>. </li>
					                  <li class="paragraph2"><mark class="red">Function estimator (\( \hat{f}\))</mark>: When the goal is to estimate the relationship between input and target variables.</li>						          
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Bias</h4>    
                                                   <ul>

					                  <li class="paragraph2">The <mark class="red">bias of an estimator</mark> is defined as:
							    \[
							     bias(\hat{\theta}_m) = \mathbb{E}(\hat{\theta}_m) -\theta,
							    \]

                                                         </li>
					                  <p class="paragraph2">where the expectation is over the data (samples of the random variable).</p>
					                  <li class="paragraph2">An estimator is said to be <mark class="red">unbiased</mark> if \( bias(\hat{\theta}_m)=0 \) .</li>
                                                          <li class="paragraph2">An estimator is said to be <mark class="red">asymptotically unbiased</mark> if
							    \[
                                                               lim_{m \leftarrow infty} bias(\hat{\theta}_m)=0 				
							    \]

                                                         </li>
 				              
						          
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Bias</h4>    
                                                   <ul>

					                  <li class="paragraph2">We compute the bias for the <mark class="red">Bernoulli distribution with parameter \( \theta \) </mark>:

							    <br>
							    <br>
							    <br>
							    \[
							     P({\bf{x}};\theta) = \theta^{\bf{x}}   (1-\theta)^{(1-{\bf{x}})}.
							    \]
							    
                                                         </li>
					                  <li class="paragraph2">Having a set of samples \( \{ x^1, \dots, x^m \} \), a common estimator for  \( \theta \) is the mean of the training samples: 
							    \[
							     \hat{\theta_m} = \frac{1}{m} \sum_{i=1}^{m} x^i. 							                                     \]

                                                         </li>

					                  <li class="paragraph2">For this estimator, we will compute  \( bias(\hat{\theta}_m) = \mathbb{E}(\hat{\theta}_m) -\theta \).</li>
                                                           
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
        		           <section> 
                                 <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Bias</h4>    
                                                   <ul>

					                  <p class="paragraph2">
							    \[
							       \begin{align}

                                                                  bias(\hat{\theta}_m) &=& \mathbb{E}(\hat{\theta}_m) -\theta \\
                                                                                       &=& \mathbb{E} \left [  \frac{1}{m} \sum_{i=1}^{m} x^i   \right ] - \theta \\
                                                                                       &=&   \frac{1}{m} \sum_{i=1}^{m} \mathbb{E} [x^i] - \theta \\
                                                                                       &=&   \frac{1}{m} \sum_{i=1}^{m} \sum_{x^i=0}^{1}  \left (  \theta^{x^i}   (1-\theta)^{(1-x^i)}   \right) - \theta \\
                                                                                       &=&   \frac{1}{m} \sum_{i=1}^{m}(\theta) - \theta \\
                                                                                       &=&   0 


							        \end{align}


							    \]

                                                         </p>
					                

                                                           
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Variance</h4>    
                                                   <ul>

					                  <li class="paragraph2">The <mark class="red">variance of an estimator</mark> is defined as:
							    \[
							     Var(\hat{\theta}_m)
							    \]

                                                         </li>
					                  <p class="paragraph2">It provides information about how much we expect the estimator to vary as a <mark class="red">function of the data sample</mark>.</p>
					                  <li class="paragraph2">The square root of the variance is called the <mark class="red">standard error</mark>.</li>
                                                          <li class="paragraph2">The <mark class="red">standard error of the mean</mark> is given by:
							    \[
                                                               SE(\hat{\mu}_m) = \sqrt{Var \left [ \frac{1}{m} \sum_{i=1}^{m} x^i   \right]} = \frac{\sigma}{\sqrt{m}}				
							    \]

                                                         </li>
 				              
						          
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
        		           <section> 
                                 <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Estimators, Bias and Variance </h3>  
                                                   <h4>Variance</h4>    
                                                   <ul>

					                  <p class="paragraph2">
							    \[
							       \begin{align}

                                                                  Var(\hat{\theta}_m) &=&  Var \left (  \frac{1}{m} \sum_{i=1}^{m} x^i   \right ) \\
                                                                                       &=&   \frac{1}{m^2} \sum_{i=1}^{m} Var (x^i) \\
                                                                                       &=&   \frac{1}{m^2} \sum_{i=1}^{m} \theta (1-\theta) \\
                                                                                       &=&   \frac{1}{m^2} m \theta (1-\theta) \\
                                                                                       &=&    \frac{1}{m}  \theta (1-\theta) \\


							        \end{align}


							    \]

                                                         </p>
					                  <li class="paragraph2">The variance of this estimator decreases as a function of \(m\).</li>

                                                           
                                                   </ul>						           
                                                   
                                                   </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section id="sec:MLE">
                                           <mark class="red"></mark>
                                          <div class="my_container">

                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        

                                                   <ul>
						          <li class="paragraph2">We consider a set of \(m\) samples \( \mathbb{X} = \{ {{\bf{x}}}^1, \dots, {\bf{x}}^m \} \) drawn independently from the true but unknown  <mark class="red">data generating distribution</mark> \(p_{data}({{\bf{x}}})\).   </li>
						          <li class="paragraph2"> \( p_{model}({\bf{x}},\theta) \) is a parametric family of probability distributions over the same space indexed by \(\theta\). </li>
						          <li class="paragraph2"> The <mark class="red">maximum likelihood estimator</mark> for \(\theta\) is defined as:
                                                          \[
                                                          \begin{align}
                                                     
                                                          \theta_{ML} &=& \underset{\theta}{\arg\max} \prod_{i=1}^{m} p_{model}(\mathbb{X},\theta)\\                                                                    
                                                                      &=& \underset{x}{\arg\max} \prod_{i=1}^{m} p_{model}({\bf{x}}^i,\theta)
                                                          \end{align}
							  \]  
                                                          </li>

                                                   
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        

                                                   <ul>
						          <li class="paragraph2">Usually,  the logarithm is used to transform the product of probabilities to a sum of logarithms: 
                                                           \[                                                         
                                                     
                                                             \theta_{ML} = \underset{\theta}{\arg\max} \sum_{i=1}^{m} log \; p_{model}({\bf{x}}^i,\theta)                                                                   
							   \]
                                                          </li>
						          <li class="paragraph2">This is an <mark class="red">equivalent optimization problem</mark> that can be also expressed as an optimization as an expectation with respect to the empirical distribution \(\hat{p}_{data}\): 
                                                           \[                                                         
                                                     
                                                             \theta_{ML} = \underset{\theta}{\arg\max} \; \mathbb{E}_{{\bf{x}} \thicksim \hat{p}_{data}} log \; p_{model}({\bf{x}},\theta)                                                                   
							   \]
                                                          </li>						          
                                                   
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
						   <h4>Kullback Leibler divergence interpretation</h4>
                                                   <ul>
						          <li class="paragraph2">The MLE can be also seen from the perspective of the  <mark class="red">Kullback-Leibler divergence</mark>  between two distributions: 
                                                           \[                                                         
                                                     
                                                             D_{KL}(\hat{p}_{data}||p_{model}) = \underset{\theta}{\arg\max} \;  \mathbb{E}_{{\bf{x}} \thicksim \hat{p}_{data}} \left [ log \; \hat{p}_{data}({\bf{x}})-log \; p_{model}({\bf{x}}) \right ]                                                                                    
							   \]
                                                          </li>

                                                           <li class="paragraph2">Since \(\hat{p}_{data}({\bf{x}})\) does not depend on the model selection, we only need to minimize:
                                                           \[                                                         
                                                     
                                                              - \underset{\theta}{\arg\max} \;  \mathbb{E}_{{\bf{x}} \thicksim \hat{p}_{data}} \left [log \; p_{model}({\bf{x}}) \right ]                                                                                    
							   \]
                                                          </li>


						          <li class="paragraph2">Therefore, minimizing the KL divergence corresponds to <mark class="red">minimizing the cross-entropy</mark> between the distributions.</li> 
						        					          
                                                   
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
                                                   <h4>Conditional probability</h4>        

                                                   <ul>
						          <li class="paragraph2"> The MLE can be also applied to the estimation of the conditional probability  \(P(y| {\bf{x}},\theta) \): 
                                                           \[                                                         
                                                     
                                                             \theta_{ML} = \underset{\theta}{\arg\max} P(\mathbb{Y}| \mathbb{X},\theta)                                                                   
							   \]

                                                          </li>
						          <li class="paragraph2">If the instances are assumed to be i.i.d., then the expression can be decomposed into: 


                                                           \[                                                         
                                                     
                                                             \theta_{ML} = \underset{\theta}{\arg\max} \sum_{i=1}^{m} log \; P(y^i|{\bf{x}}^i,\theta).                                                                  
							   \]                                                          
                                                          </li>						          
                                                   
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                  <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
                                                   <h4>Linear regression as an example</h4>        

                                                   <ul>
						          <li class="paragraph2">We assume that in an infinitively large training set, there might be several training examples with <mark class="red"> the same input value \( {\bf{x}} \) but different values of \(y\)</mark>.</li> 
						          <li class="paragraph2">The goal of the learning algorithm is to <mark class="red">fit the distribution \( p(y|{\bf{x}}) \)</mark> to all of those different  \(y\) values compatible with \( {\bf{x}} \). </li> 
						          <li class="paragraph2">We assume that \( p(y|{\bf{x}}) \)  follows a <mark class="red">Normal distribution</mark>  \( p(y|{\bf{x}})= \mathcal{N}(y;\hat{y}({\bf{x}},w), \sigma^2)\). </li> 

                                                          <li class="paragraph2"> The function \(\hat{y}({\bf{x}},w) \) gives the prediction of the <mark class="red">mean of the Gaussian</mark>. </li> 

                                                          <li class="paragraph2"> The variance is fixed to some constant \(\sigma^2\). </li> 
                                                   
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                  <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
                                                   <h4>Linear regression as an example</h4>        

                                                   <ul>
                                                          <li class="paragraph2"> For the Gaussian distribution: 
                                                           \[                                                         
                                                                                                               
							    \begin{align}
                                                               P(y| {\bf{x}},\theta)      &=& \mathcal{N}(y;\hat{y}({\bf{x}},w), \sigma^2) \\
                                                                                          &=& \frac{1}{\sqrt{2\pi \sigma^2}} e^\frac{(y-\hat{y}({\bf{x}},w))^2}{\sigma^2}
							    \end{align}                                                          
							   \]

                                                          </li>
						          <li class="paragraph2">Therefore, the conditional log-likelihood is given by:
                                                           \[                                                          
							    \begin{align}
                                                               \sum_{i=1}^{m} log \; P(y^i|{\bf{x}}^i,\theta) &=& \sum_{i=1}^{m} log \; \frac{1}{\sqrt{2\pi \sigma^2}} e^\frac{(y^i-\hat{y}({\bf{x}}^i,w))^2}{\sigma^2} \\
                                                               &=& -m \, log(\sigma) - \frac{m}{2} log(2\pi) - \sum_{i=1}^{m} \frac{||\hat{y}^i-y^i||^2}{2\sigma^2}
							    \end{align}
                                                           \]  
                                                          </li> 
						       
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                  <section>
                                          <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
                                                   <h4>Linear regression as an example</h4>        

                                                
						          <p class="paragraph2">Maximizing the conditional likelihood
                                                           \[                                                          
							   
                                                               \sum_{i=1}^{m} log \; P(y^i|{\bf{x}}^i,\theta) = -m \, log(\sigma) - \frac{m}{2} log(2\pi) - \sum_{i=1}^{m} \frac{||\hat{y}^i-y^i||^2}{2\sigma^2}

                                                           \]  
                                                          </p> 
                                                          <p class="paragraph2">is equivalent to maximizing the mean squared error:
                                                           \[                                                          
							   
                                                               MSE_{train} = \frac{1}{m} \sum_{i=1}^{m} ||\hat{y}^i-y^i||^2

                                                           \]  
                                                          </p> 
						       
                                                   </ul>						           
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                  <section>
                                                 <mark class="red"></mark>
                                                 <div class="my_container">
                                                   <h3>Maximum Likelihood Estimation (MLE)  </h3>        
                                                   <h4>Properties</h4>       
                                                   <ul>

						          <li class="paragraph2"> <mark class="red">Under appropriate conditions</mark>, the maximum likelihood estimator has the  <mark class="red">property of consistency</mark>, i.e., as the number of training examples approaches infinity, the ML estimator of a parameter converges to the true value of the parameter.  </li>
						          <li class="paragraph2"> These conditions are:
                                                              <ol>
       						                    <li class="paragraph2"> The true distribution \(p_{data}\) <mark class="red">must lie within the model family</mark> \(p_{model}(.,\theta) \). Otherwise, no estimator can recover  \(p_{data}\). </li>
                                                                   <li class="paragraph2"> The true distribution \(p_{data}\) <mark class="red">must correspond to exactly one value of \( \theta \)</mark>. Otherwise, maximum likelihood can recover the correct \(p_{data}\), but will not be able to determine which value of \( \theta \) was used by the data generating processing. </li>                                                 </ol>
                                                         </li>						         
                                                   </ul>		
                                                       <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>
		
 				           
                               		           </div>                                                       
                                                   <aside class="notes">
  						   
                                            	   </aside>
        			    </section>
                                       <section id="sec:Bayesian_Approach">                   
                                                 <mark class="red"></mark>
                                              <div class="container">
                                                 <h3>Differences between frequentist and Bayesian approaches</h3>
                                                  <span class="fragment">
                                                  <div class="right">
                                                  <h4>Bayesian approach</h4>
                                                      <ul>      
                                                         <li class="paragraph2">  The dataset is directly observed and therefore is not random.  </li>
                                                         <li class="paragraph2">  The true parameter  \(\theta\) is <mark class="red"> unknown</mark> or <mark class="red">uncertain</mark>  and thus is represented as a random variable. </li>


                                                       <li class="paragraph2"> The knowledge about \(\theta\), before observing the data, is represented using a <mark class="red"> prior probability distribution  \( p(\theta)\)</mark>.   </li>	
  					   
                                                      <ul/>                                                      
                          		          </div>
                                                  </span>           
                                      
                                                  <div class="left">   
                                                      <h4>Frequentist approach</h4>
                                                      <ul>      

                                                       <li class="paragraph2"> MLE follows a frequentist approach. </li>
                                                       <li class="paragraph2"> The true parameter \(\theta\) is <mark class="red">fixed but unknown</mark>. </li>
                                                       <li class="paragraph2"> The point estimate  \( \hat{\theta} \) is a  <mark class="red">random variable</mark>. </li>

                                                       <li class="paragraph2"> It can be more convenient than Bayesian approaches when the number of training samples is large. </li>

                                                      <ul/>                                                     
                                                  </div>         
                                                  </div>
                                                      <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>

                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	      </section>
                                    <section>
                                                 <mark class="red"></mark>
                                                 <div class="my_container">
                                                   <h3>Bayesian approach</h3>        
                                                   <h4>Bayes rule</h4>       
                                                   <ul>

						          <li class="paragraph2">The way to update our belief about \(\theta\) using a set of data samples  \( \{ {\bf{x}}^1, \dots, {\bf{x}}^m \} \)  is by combining the the data likelihood  \( p({\bf{x}}^1, \dots, {\bf{x}}^m |\theta)\) with the prior via the   <mark class="red">Bayes rule</mark>: 
                                                 <br>
                                                 <br>
                                                 <br>

                                                  \[

                                                    p(\theta| {\bf{x}}^1, \dots, {\bf{x}}^m) = \frac{ p({\bf{x}}^1, \dots, {\bf{x}}^m|\theta)p(\theta)} {p({\bf{x}}^1, \dots, {\bf{x}}^m)}
                                         
                                                  \]


                                                           </li>	
						          <li class="paragraph2">The prior has an influence by shifting  the probability mass density towards region of the parameter space that are preferred <mark class="red"> a priori</mark>. </li>	
				         
                                                   </ul>	
                                                      <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>
	
 				           
                               		           </div>                                                       
                                                   <aside class="notes">
  						   
                                            	   </aside>
        			    </section>
                                    <section>
                                                 <mark class="red"></mark>
                                                 <div class="my_container">
                                                   <h3>Maximum A Posteriori (MAP) Estimation</h3>   
                                                   <ul>

   <li class="paragraph2"> Even using the Bayesian approach, in some scenarios is convenient to compute a  <mark class="red">point estimate</mark>. </li>	

						          <li class="paragraph2">The  <mark class="red">MAP estimate</mark> chooses the point of maximal posterior probability (or probability density for continuous \( \theta \)): 
                                                 <br>
                                                 <br>
                                                 <br>

                                                  \[

                                                     \theta_{MAP} =  \underset{\theta}{\arg\max} \;p(\theta|{\bf{x}}) = \underset{\theta}{\arg\max} \; log \, p({\bf{x}}|\theta) + log \, p(\theta)                                          
                                                  \]


                                                           </li>	
						          <li class="paragraph2">The MAP estimate has the advantage of leveraging information that is brought by the prior and cannot be found in the training data. </li>	
				         
                                                   </ul>	
                                                      <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>			
 				           
                               		           </div>                                                       
                                                   <aside class="notes">
  						   
                                            	   </aside>
        			    </section>

                                    <section id="sec:Regularization">
                                           <mark class="red"></mark> 
                                          <div class="my_container">
                                                   <h3>Regularization</h3>      
                                                   <h4>Important concepts</h4> 
                                                      <ol>

						          <li class="paragraph2"> <mark class="red">Generalization</mark>: The ability of a model to perform well on previously unobserved inputs.  </li>
						          <li class="paragraph2">The <mark class="red">generalization error</mark> is defined as the expected value of the error on a new input.  </li>
						          <li class="paragraph2">The expected error is usually estimated using a <mark class="red">test set of examples</mark>. </li>
                                                  <span class="fragment">
						          <li class="paragraph2"><mark class="red">Underfitting</mark>: Occurs when the model is not able to obtain a sufficiently low error value on the <mark class="red">training set</mark>.  </li>
						          <li class="paragraph2"><mark class="red">Overfitting</mark>: Occurs when the gap between the training error and test error is too large.  </li>
                                                  </span>
						                                     
                                                   </ol>						           
                                                    <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                 <section>
                                           <mark class="red"></mark> 
                                          <div class="my_container">
                                                   <h3>Regularization</h3>                                                                                                                 <ol>

						          <li class="paragraph2">Regularization is any modification we make to a learning algorithm that is intended to <mark class="red">reduce its generalization error</mark> but not its training error. </li>
						          <li class="paragraph2"> Regularization of an estimator works by <mark class="red">trading increased bias for reduced variance</mark>. </li>                                  
                                                  <span class="fragment">
						          <li class="paragraph2">An effective regularizer is one that reduces variance signifincantly by not overly increasing the bias.</li>
						          <li class="paragraph2">The goal is to achieve a model of sufficient complexity to <mark class="red">fit the training data</mark> but <mark class="red">able to generalize</mark> to unseen data. </li>                                                                    </span>                                 
                                                   </ol>						           
                                                    <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 

                                   <section>
                                          <div class="my_container">
                                           <mark class="red"></mark> 
                                                   <h3>Regularization</h3>                                                   
                                                   <h4>Parameter norm penalties</h4>
                                                   <ul>

						          <li class="paragraph2"><mark class="red">Parameter regularization</mark> is implemented by adding a <mark class="red">parameter norm penalty</mark>  \(\Omega(\theta)\) to the objective function \(J\). </li>       

   <li class="paragraph2">The  <mark class="red">regularized objective function</mark> \(\tilde{J}\) is defined as: 
                                                 <br>
                                                 <br>
                                                 <br>
                                                  \[

                                                     \tilde{J}(\theta;{\bf{X}},y)  = J(\theta;{\bf{X}},y) + \alpha \, \Omega(\theta),                                
                                                  \]


                                                           </li>          

						          <p class="paragraph2">where \(\alpha \in [0,\infty] \) is a hyperparameter that weights the relative contribution of  \(\Omega(\theta)\) relative to  \( J(\theta;{\bf{X}},y) \).</p>                                  
                                                   </ul>						           
                                                    <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <div class="my_container">
                                           <mark class="red"></mark> 
                                                   <h3>Regularization</h3>                                                   
                                                   <h4>\(L^2\) parameter regularization</h4>
                                                   <ul>

						          <li class="paragraph2"> The \(L^2\) parameter regularization adds a regularization term of the form  <mark class="red"> \(\Omega(\theta) =\frac{1}{2} \Vert \theta \Vert_2^2 \)</mark>. </li>       

                                                         <li class="paragraph2"> In neural networks, usually regularization is applied to the weights of network, i.e.,  <mark class="red">\( \theta = w \)</mark>. </li> 

                                                         <li class="paragraph2"> A regularized model of this type has the form:

                                                 <br>
                                                 <br>
                                                 <br>
                                                  \[

                                                     \tilde{J}(\theta;{\bf{X}},y)  = J(\theta;{\bf{X}},y) + \frac{\alpha}{2} w^{\top}w.                                
                                                  \]


                                                           </li>          

						                                    
                                                   </ul>						           
                                                    <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <div class="my_container">
                                           <mark class="red"></mark> 
                                                   <h3>Regularization</h3>                                                   
                                                   <h4>\(L^1\) parameter regularization</h4>
                                                   <ul>

						          <li class="paragraph2"> The   <mark class="red"> \(L^1\) parameter regularization</mark>  adds a regularization term of the form:   

                                                <br>
                                                 <br>
                                                 <br>
                                                  \[
                                                      \Omega(\theta) = \Vert \theta \Vert_1 = \sum_i |w_i|.
                                                     
                                                  \]

                                                           </li>      

                                                         

                                                         <li class="paragraph2"> <mark class="red">A regularized model</mark> of this type has the form:

                                                 <br>
                                                 <br>
                                                 <br>
                                                  \[

                                                     \tilde{J}(\theta;{\bf{X}},y)  = J(\theta;{\bf{X}},y) + \alpha \Vert \theta \Vert_1                                
                                                  \]


                                                           </li>          

                                                        <li class="paragraph2"> \(L^1\) parameter regularization tends to produce <mark class="red">more  sparse solutions</mark>, i.e., some parameters have an optimal value of zero. 						                                    
                                                   </ul>						           
                                                    <p class="paragraph2"> I. Goodfellow and Y. Bengio and A. Courville. <a href="http://www.deeplearningbook.org/"> Deep Learning.</a>. MIT Press. 2016. </p>	                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 


                                          <section  id="sec:Gradient">
                                                    <h2> Gradient optimization methods <h2>                                                                                
                                                      <p class="paragraph2">
    <img src="http://2.bp.blogspot.com/-q6l20Vs4P_w/VPmIC7sEhnI/AAAAAAAACC4/g3UOUX2r_yA/s1600/s25RsOr%2B-%2BImgur.gif"  height="500" width="900">
                                                      
                                                      </p>           

                                                    <p class="paragraph2">Images credit: <a href="https://twitter.com/alecrad">  Alec Radford.</a></p>	      
                                                   <aside class="notes">  						   
                                                        <img src="https://i.imgur.com/NKsFHJb.gif"  height="400" width="400">
                                            	   </aside>
 
                                          </section>


 
                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Gradient</h3>                                                   
                                                   <h4>Definition and interpretation</h4>
		 
                                              <p class="paragraph2"> The gradient of a function \( J(\theta_1,\dots,\theta_d) \) is a vector-value function defined as: 
							    <br>
							    <br>
							    <br>
							    
                                                \[
                                                    \nabla J(\theta_1,\dots,\theta_d) = < \frac{\partial J}{\partial \theta_1}(\theta_1,\dots,\theta_d), \dots,\frac{\partial J}{\partial \theta_d}(\theta_1,\dots,\theta_d)>
                                                \] 
                                              </p>         
                          		 
                                              <ol>

						   <li class="paragraph2">The gradient of a multi-variable function has <mark class="red">a component for each direction</mark>. </li>
						   <li class="paragraph2">The gradient points in the  <mark class="red">direction of greatest increase</mark>. </li>
                                                   
                                               </ol>						           
                                                   
                                                               
                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Gradient</h3>                                                   
                                                   <h4>Example</h4>	 
                                            
                          		 
                                              <ol>

						   <li class="paragraph2"> Let us consider a function defined in the three dimensional space that associates a temperature to each point of a three dimensional space. </li>	
						   <li class="paragraph2"> In this example, the gradient gives which is the direction in which an increase in the temperature would be the greatest.  </li>						
					
                                                   
                                               </ol>						           
                                                   
                                                               
                                                   
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						      A classical example is a function that associates a temperature to each point of a three dimensional space.
                                                      In this example, the gradient gives which is the direction in which an increase in the temperature would be the greatest. 
                                            	   </aside>
        		           </section> 
                                   <section  id="sec:GradientDescent">
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Gradient descent</h3>                                                   
                                                   <h4>Finding the optimum of the loss function</h4>

						          <p class="paragraph2">  <mark class="red">Gradient descent</mark>: A local minimization method based on updating the parameters of a function \( J(\theta_1,\dots,\theta_d) \) in the opposite direction to its gradient.  </p>

						          <p class="paragraph2"> A parameter \( \mu \) is used to indicate the <mark class="red">learning rate of the algorithm</mark> (the size of the step taken to reach to local optimum)    </p>						         
					
                                                    <p class="paragraph2">S. Ruder.<a href="https://arxiv.org/abs/1609.04747"> An overview of gradient descent optimization algorithms.</a> arXiv preprint arXiv:1609.04747. 2016.</p>                                                                                                
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section  id="sec:GradientDescent">
                                   <mark class="red"></mark>
                                          <div class="my_container">
                                                   <h3>Gradient descent</h3>                                                   
                                                   <h4>Finding the optimum of the loss function</h4>

						          <p class="paragraph2"> Gradient descent algorithms can be grouped in <mark class="red">three classes</mark> according to the way the gradient is used for the updates: </p>
    
                                                   <ol>
						          <li class="paragraph2">Batch gradient descent.</li>
						          <li class="paragraph2">Stochastic gradient descent (SGD).</li>
						          <li class="paragraph2">Mini-batch gradient descent.</li>                                                 
                                                   </ol>
					
                                                    <p class="paragraph2">S. Ruder.<a href="https://arxiv.org/abs/1609.04747"> An overview of gradient descent optimization algorithms.</a> arXiv preprint arXiv:1609.04747. 2016.</p>                                                                                                
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <div class="my_container">
                                                   <h3>Gradient descent variants</h3>                                                   
                                                   <h4>Batch gradient descent</h4>

						   <p class="paragraph2"> To perform one parameter update, computes the gradient of \(J\) for all the points in the dataset as: 
							    <br>
							    <br>
							    <br>
							    
                                                    <mark class="red">
                                                    \[
                                                        \theta = \theta - \mu  \nabla_{\theta} J(\theta)
                                                     \] 
                                                    </mark>
                                                   </p>

                                                   <ol>
					                <li class="paragraph2"> Guaranteed to converge to the global minimum for convex functions and to local minimum for non-convex functions. </li>			        
                                                  
						        <li class="paragraph2"> Not very efficient since for performing a single update the gradients of the whole dataset are evaluated. </li>
                                                   <ol/>
					                                                                                            
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <div class="my_container">
                                                   <h3>Gradient descent variants </h3>                                                   
                                                   <h4>Stochastic gradient descent (SGD)</h4>

						   <p class="paragraph2"> A parameter update is performed for each point \(x^i\) and label \(y^i\) as:                                      

							    <br>
							    <br>
							    <br>
							    
                                                     <mark class="red">
                                                    \[
                                                        \theta = \theta - \mu  \nabla_{\theta} J(\theta;x^i,y^i)
                                                     \] 
                                                    </mark>
                                                   </p>

                                                   <ol>
					                <li class="paragraph2"> Usually much faster than batch gradient descent. </li>				        
					  	        <li class="paragraph2"> Can be used to learn online.</li>

						        <li class="paragraph2"> Convergence to local minimum is not guaranteed.</li>
                                                   <ol/>
					                                                                                            
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                          <div class="my_container">
                                                   <h3>Gradient descent variants</h3>                                                                                               <h4>Mini-batch gradient descent</h4>

						   <p class="paragraph2"> A parameter update is performed for each mini-batch of \(n\) points \( (x^i,\dots,x^{i+n})\) and labels \((y^i,\dots,y^{i+n})\) as:
							    <br>
							    <br>
							    <br>
							    

                                                     <mark class="red">
                                                    \[
                                                        \theta = \theta - \mu  \nabla_{\theta} J(\theta; (x^i,\dots,x^{i+n}),(y^i,\dots,y^{i+n}))
                                                     \] 
                                                    </mark>
                                                   </p>
                                                   <ol>
					                 <li class="paragraph2"> Combines characteristics of batch gradient descent and SGD. </li>				        
						         <li class="paragraph2"> Can make use of highly optimized matrix optimizations.</li>

						         <li class="paragraph2"> Nevertheless, it does not guarantee a good convergence.</li>
						         <li class="paragraph2"> Very sensitive to the learning rate \( \mu \).</li>

						         <li class="paragraph2"> It can be trapped in local optima, particularly saddle points.</li>
                                                   <ol/>

					                                                                                            
                          		 </div>                                                          
                                                   
                                                   <aside class="notes">
  						     
                                            	   </aside>
        		           </section> 
                                   <section>
                                             <h3>Gradient descent variants</h3>     
                                             <h4>Advanced gradient descent methods</h4>
                                              <div class="container">
                                                  <span class="fragment">
                                                  <div class="right">
                                                  <h4>Nesterov accelerated gradient (NAG)</h4>
                                                      <ul>  

      		           			           <li class="paragraph2"> Computes an approximate prediction of the parameters in order to calculate the gradient w.r.t. the approximate future position of the parameters.</li>
                                                           <li class="paragraph2"> The updates are defined as: 
							    <br>
							    <br>
							    <br>
							    
                                                           <mark class="red">
                                                             \[
                                                                v_t = \gamma v_{t-1} +  \mu  \nabla_{\theta} J(\theta - \gamma v_{t-1}) \\
                                                                \theta = \theta - v_t
                                                             \] 
                                                           </mark>

                                                          </li>
                                                     </ul>                                                                                                 
                           		          </div>
                                                  </span>
                                                 
                                                  <div class="left">   
                                                  <h4>Momentum</h4>
                                                      <ul>  
                                                           <li class="paragraph2"> A fraction <mark class="red"> \( \gamma \)  </mark>of the update vector of the past time step is added to the current vector as: 
							    <br>
							    <br>
							    <br>
							    
                                                           <mark class="red">
                                                             \[
                                                                v_t = \gamma v_{t-1} +  \mu  \nabla_{\theta} J(\theta) \\
                                                                \theta = \theta - v_t
                                                             \] 
                                                           </mark>

                                                          </li>

      		           			           <li class="paragraph2"> Helps accelerate SGD in the relevant directions and dampens oscillations.</li>
      		           		         
      		           	                                                           
                                                     </ul> 

                                                  </div>    
                                                   
                                              </div>  
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                             	   </section>
                                   <section>
                                             <h3>Gradient descent variants</h3>     
                                             <h4>Other advanced gradient descent methods</h4>
                                              <div class="container">
                                                  <span class="fragment">
                                                  <div class="right">
                                                  <h4>Adagrad</h4>
                                                  <h4>Adadelta</h4>
                                                  <h4>RMSprop</h4>
                                                  <h4>Adam</h4>       
                                                  <h4>AdaMax</h4>
                                                  <h4>Nadam</h4>                                              

                           		          </div>
                                                  </span>
                                                 
                                                  <div class="left">   
                                                  <h4>Characteristics</h4>
                                                      <ul>  
                                                          <li class="paragraph2"> Adapt the learning rate of parameters (similar to annealing schedules). </li>
                                                          <li class="paragraph2"> Can use a different learning rate for each parameter. </li>      	
                                                          <li class="paragraph2"> Some restrict the window of accumulated past gradients to some fixed size \( w \). </li>
                                                          <li class="paragraph2"> Keep exponentially decaying average of past gradients.  </li>   	           		         
      		           	                                                           
                                                     </ul> 

                                                  </div>    
                                                   
                                              </div>  
                                                   <aside class="notes">
                                                       What is ML? 
                                            	   </aside>

                              	         </section>


                                          <section>
                                                    <h2> Gradient optimization methods <h2>                                                                                
                                                      <p class="paragraph2">
                                                         <img src="http://cs231n.github.io/assets/nn3/opt1.gif"  height="400" width="400">
                                                         <img src="http://cs231n.github.io/assets/nn3/opt2.gif"  height="400" width="400">
                                                      </p>           

                                                    <p class="paragraph2">Images credit: <a href="https://twitter.com/alecrad">  Alec Radford.</a></p>	      
                                          </section>
         	                </section>  
 
			</div>
		</div>




		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			Reveal.initialize({
				history: true,
				transition: 'linear',

				math: {
					// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
					config: 'TeX-AMS_HTML-full'
				},

				dependencies: [
                                        { src: 'lib/js/fullscreen-img.js' },
					{ src: 'lib/js/classList.js' },
					{ src: 'plugin/math/math.js', async: true }

				]
			});

		</script>

	</body>
</html>
