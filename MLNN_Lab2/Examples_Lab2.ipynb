{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputers\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = np.array([[ 'NaN',   7,     6],\n",
    "                    [  5   ,  89,    13],\n",
    "                    [ 23   ,  12,   213],\n",
    "                    [  2   ,  87, 'NaN'],\n",
    "                    [  8   , 101,    71],\n",
    "                    [ 13   ,'NaN',    20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10.2    7.     6. ]\n",
      " [   5.    89.    13. ]\n",
      " [  23.    12.   213. ]\n",
      " [   2.    87.    64.6]\n",
      " [   8.   101.    71. ]\n",
      " [  13.    59.2   20. ]]\n"
     ]
    }
   ],
   "source": [
    "mean_imputer = Imputer(missing_values='NaN',strategy=\"mean\",axis=0)\n",
    "mean_imputer.fit(my_data)\n",
    "imputed_data = mean_imputer.transform(my_data)\n",
    "print(imputed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization or scalers\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.64412211e-16  -1.39837036e+00  -8.26676029e-01]\n",
      " [ -7.74024378e-01   7.98303387e-01  -7.27926333e-01]\n",
      " [  1.90529078e+00  -1.26442684e+00   2.09349356e+00]\n",
      " [ -1.22057690e+00   7.44725979e-01  -2.00473941e-16]\n",
      " [ -3.27471852e-01   1.11976784e+00   9.02854367e-02]\n",
      " [  4.16782357e-01   1.90345192e-16  -6.29176637e-01]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(imputed_data)\n",
    "scaled_data = scaler.transform(imputed_data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binarize\n",
    "from sklearn.preprocessing import binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  1.]\n",
      " [ 1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "binarized_data = binarize(scaled_data)\n",
    "print(binarized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature selection (Filtering using ANOVA F_Test (f_classif))\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.64412211e-16  -1.39837036e+00]\n",
      " [ -7.74024378e-01   7.98303387e-01]\n",
      " [  1.90529078e+00  -1.26442684e+00]\n",
      " [ -1.22057690e+00   7.44725979e-01]\n",
      " [ -3.27471852e-01   1.11976784e+00]\n",
      " [  4.16782357e-01   1.90345192e-16]]\n"
     ]
    }
   ],
   "source": [
    "feature_selection = SelectKBest(f_classif,k=2)\n",
    "my_class = np.array([1,1,1,0,0,0])\n",
    "new_features = feature_selection.fit_transform(scaled_data, my_class)\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction (dimensionality reduction)\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31130824  1.57076484]\n",
      " [-1.32366387 -0.04615306]\n",
      " [ 3.04183574 -0.59333748]\n",
      " [-1.19300064 -0.52826786]\n",
      " [-0.76851544 -0.85238305]\n",
      " [-0.06796403  0.44937662]]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "reduced_data = pca.transform(scaled_data)\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating machine learning pipelines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3d97a5ceeb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imputer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_imputer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'standarize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'reduce_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'svc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "my_pipeline = Pipeline([('imputer', mean_imputer),('standarize', scaler), ('reduce_dim', pca), ('clf',lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with our pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted_class = cross_val_predict(my_pipeline,my_data,my_class)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the pipeline is:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the accuracy of the pipeline\n",
    "from sklearn import metrics\n",
    "pipeline_accuracy = metrics.accuracy_score(my_class,predicted_class) \n",
    "print(\"The accuracy of the pipeline is: \",pipeline_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline learned by tpot is:\n"
     ]
    }
   ],
   "source": [
    "# TPOT: Automaticly finding pipelines\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "my_tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=16)\n",
    "my_tpot.fit(features=my_data, target=my_class)\n",
    "print(\"The pipeline learned by tpot is:\")\n",
    "my_tpot.fitted_pipeline_.steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[('robustscaler',\n",
    "  RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
    "         with_scaling=True)),\n",
    " ('logisticregression',\n",
    "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "            verbose=0, warm_start=False))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
